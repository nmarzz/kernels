{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digits(Dataset):\n",
    "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode='train', transforms=None):\n",
    "        digits = load_digits()\n",
    "        if mode == 'train':\n",
    "            self.data = digits.data[:1000].astype(np.float32)\n",
    "        else:\n",
    "            self.data = digits.data[1000:].astype(np.float32)\n",
    "\n",
    "        self.targets = digits.target\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample, target    \n",
    "    \n",
    "def mnist_loader(batch_size: int) -> None:\n",
    "    train_set = datasets.MNIST('data', train=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]), download=True)\n",
    "    val_set = datasets.MNIST('data', train=False,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.1307,), (0.3081,))\n",
    "                             ]), download=True)\n",
    "    \n",
    "    train_set = torch.utils.data.Subset(train_set, range(5000))\n",
    "    val_set = torch.utils.data.Subset(val_set, range(1000))\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=batch_size, num_workers=8,\n",
    "        pin_memory=True, shuffle= True)\n",
    "\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory= True)\n",
    "\n",
    "    return train_loader, val_loader    \n",
    "\n",
    "def digits_loader(batch_size:int):\n",
    "    transform = transforms.Lambda(lambda x: 2. * (x / 17.) - 1.)\n",
    "    train_data = Digits(mode='train', transforms=transform)\n",
    "    test_data = Digits(mode='test', transforms=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory= True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader  \n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    ''' A basic 3 layer MLP '''\n",
    "\n",
    "    def __init__(self, input_dim: int, num_classes: int, hidden_dim: int = 32) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x  \n",
    "    \n",
    "    def features(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(model, loader):\n",
    "    feats = []\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = np.zeros((0, hidden_size))\n",
    "    labels = np.zeros((0))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data = data.to(device)            \n",
    "            embs = model.features(data)\n",
    "            embeddings = np.concatenate((embeddings, embs.cpu().numpy()))\n",
    "            labels = np.concatenate((labels, target))\n",
    "\n",
    "    return embeddings, labels        \n",
    "\n",
    "def train_epoch(epoch, train_loader):    \n",
    "    model.train()      \n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x,y = x.to(device),y.to(device)        \n",
    "        optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = loss_function(output,y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "    \n",
    "        \n",
    "def test_epoch(epoch, test_loader, output_epochs = 10):        \n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    N = 0\n",
    "    for data, labels in test_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        logits = model(data)\n",
    "        \n",
    "        loss_t = loss_function(logits, labels)\n",
    "        acc_t = 100 * torch.sum(torch.argmax(logits,dim = 1) == labels) / len(labels)\n",
    "\n",
    "\n",
    "        loss += loss_t.item()\n",
    "        acc += acc_t\n",
    "        N += data.shape[0]\n",
    "\n",
    "    loss = loss / N\n",
    "    acc = acc / len(test_loader)\n",
    "    \n",
    "    if epoch % output_epochs == 0:\n",
    "        print(f'Epoch: {epoch}, CE = {loss}, ACC = {acc}')\n",
    "  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, CE = 0.0168315339833498, ACC = 82.6171875\n",
      "Epoch: 1, CE = 0.013591308519244194, ACC = 85.64453125\n",
      "Epoch: 2, CE = 0.012158217549324035, ACC = 88.8671875\n",
      "Epoch: 3, CE = 0.011766072049736977, ACC = 90.0390625\n",
      "Epoch: 4, CE = 0.014020623974502086, ACC = 86.42578125\n"
     ]
    }
   ],
   "source": [
    "dataset = 'mnist'\n",
    "batch_size = 32\n",
    "\n",
    "if dataset == 'digits':\n",
    "    train_loader, test_loader = digits_loader(batch_size = batch_size)\n",
    "    data_width = 8\n",
    "    data_dim = data_width**2\n",
    "    num_classes = 10\n",
    "    output_epochs = 1\n",
    "elif dataset == 'mnist':\n",
    "    train_loader, test_loader = mnist_loader(batch_size = batch_size)\n",
    "    data_width = 28\n",
    "    data_dim = data_width**2\n",
    "    num_classes = 10\n",
    "    output_epochs = 1    \n",
    "\n",
    "    \n",
    "## Define hyperparameters\n",
    "hidden_size = 32 # size of layers in model\n",
    "lr = 0.01 \n",
    "num_epochs = 5\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "### Define model and optimizer\n",
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "model = MLP(input_dim = data_dim, num_classes = num_classes, hidden_dim = hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "best_loss = 1000\n",
    "epochs_since_improvement = 0\n",
    "for epoch in range(num_epochs):    \n",
    "    train_epoch(epoch, train_loader)\n",
    "    test_loss = test_epoch(epoch, test_loader, output_epochs = output_epochs)\n",
    "    if test_loss < best_loss:\n",
    "        best_model = deepcopy(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader\n",
      "tensor([0, 3, 6, 3, 6, 0, 4, 8, 1, 0, 3, 7, 1, 0, 8, 8, 9, 8, 0, 4, 0, 7, 8, 6,\n",
      "        0, 3, 9, 4, 4, 3, 2, 7])\n",
      "tensor([0, 3, 6, 3, 6, 0, 4, 8, 1, 0, 3, 7, 1, 0, 8, 8, 9, 8, 0, 4, 0, 7, 8, 6,\n",
      "        0, 3, 9, 4, 4, 3, 2, 3])\n",
      "tensor(0.1948, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Test Loader\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 8, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1, 3, 1])\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1, 3, 1])\n",
      "tensor(0.3761, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('Train Loader')\n",
    "for x,y in train_loader:\n",
    "    output = model(x)\n",
    "    print(torch.argmax(output,dim = 1))\n",
    "    print(y)\n",
    "    print(loss_function(output, y))\n",
    "    break\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('Test Loader')    \n",
    "for x,y in test_loader:\n",
    "    output = model(x)\n",
    "    print(torch.argmax(output,dim = 1))\n",
    "    print(y)\n",
    "    print(loss_function(output, y))\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature kernels\n",
    "\n",
    "# Train set\n",
    "feats_train, labels_train = get_features(model,train_loader)    \n",
    "FK = feats_train @ feats_train.transpose()\n",
    "\n",
    "# Test set\n",
    "feats_test, labels_test = get_features(model,test_loader)    \n",
    "FK_test = feats_test @ feats_test.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 29s, sys: 2min 22s, total: 6min 52s\n",
      "Wall time: 3min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's decompose the feature kernel. This is kernel PCA on the data with a kernel learned via features\n",
    "U,S,V = np.linalg.svd(FK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.31 s, sys: 3.57 s, total: 6.89 s\n",
      "Wall time: 3.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Decompose the test kernel just cause we can\n",
    "U_test,S_test,V_test = np.linalg.svd(FK_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.16530919e+06, 5.11760562e+05, 4.65160720e+05, 4.28413869e+05,\n",
       "       3.65938303e+05, 2.73957573e+05, 2.27790166e+05, 1.60870160e+05,\n",
       "       1.46898906e+05, 6.55960637e+04, 4.09559756e+04, 3.99963090e+04,\n",
       "       3.00615172e+04, 2.69785148e+04, 2.40713948e+04, 2.11203608e+04,\n",
       "       1.53656209e+04, 1.39918386e+04, 1.05963567e+04, 8.74369220e+03,\n",
       "       7.01546876e+03, 6.35805428e+03, 4.54111243e+03, 4.11854409e+03,\n",
       "       3.30611619e+03, 2.51226504e+02, 3.53999143e-01, 1.09344492e-03])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[0:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.14603530e+05, 8.54345974e+04, 8.10318090e+04, 6.86575938e+04,\n",
       "       5.16320201e+04, 4.57206065e+04, 3.97376332e+04, 2.36400533e+04,\n",
       "       2.02252872e+04, 9.89092232e+03, 5.55541989e+03, 5.25442143e+03,\n",
       "       4.97344186e+03, 4.34181484e+03, 3.87960943e+03, 3.68633002e+03,\n",
       "       2.84022135e+03, 2.38076956e+03, 1.63306511e+03, 1.35243086e+03,\n",
       "       1.21016942e+03, 1.08468327e+03, 7.74765737e+02, 5.19531785e+02,\n",
       "       1.08372090e+02, 2.32772694e+00, 7.25967818e-02, 1.84893981e-10])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_test[0:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
