{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digits(Dataset):\n",
    "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode='train', transforms=None):\n",
    "        digits = load_digits()\n",
    "        if mode == 'train':\n",
    "            self.data = digits.data[:1000].astype(np.float32)\n",
    "        else:\n",
    "            self.data = digits.data[1000:].astype(np.float32)\n",
    "\n",
    "        self.targets = digits.target\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample, target    \n",
    "    \n",
    "def mnist_loader(batch_size: int) -> None:\n",
    "    train_set = datasets.MNIST('data', train=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]), download=True)\n",
    "    val_set = datasets.MNIST('data', train=False,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.1307,), (0.3081,))\n",
    "                             ]), download=True)\n",
    "    \n",
    "    train_set = torch.utils.data.Subset(train_set, range(5000))\n",
    "    val_set = torch.utils.data.Subset(val_set, range(1000))\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=batch_size, num_workers=4,\n",
    "        pin_memory=True, shuffle= True)\n",
    "\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory= True)\n",
    "\n",
    "    return train_loader, val_loader    \n",
    "\n",
    "def digits_loader(batch_size:int):\n",
    "    transform = transforms.Lambda(lambda x: 2. * (x / 17.) - 1.)\n",
    "    train_data = Digits(mode='train', transforms=transform)\n",
    "    test_data = Digits(mode='test', transforms=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory= True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader  \n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    ''' A basic 3 layer MLP '''\n",
    "\n",
    "    def __init__(self, input_dim: int, num_classes: int, hidden_dim: int = 32) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes,bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x  \n",
    "    \n",
    "    def features(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(model, loader):\n",
    "    feats = []\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = np.zeros((0, hidden_size))\n",
    "    labels = np.zeros((0))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data = data.to(device)            \n",
    "            embs = model.features(data)\n",
    "            embeddings = np.concatenate((embeddings, embs.cpu().numpy()))\n",
    "            labels = np.concatenate((labels, target))\n",
    "\n",
    "    return embeddings, labels        \n",
    "\n",
    "def train_epoch(epoch, train_loader):    \n",
    "    model.train()      \n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x,y = x.to(device),y.to(device)        \n",
    "        optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = loss_function(output,y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "    \n",
    "        \n",
    "def test_epoch(epoch, test_loader, output_epochs = 10):        \n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    N = 0\n",
    "    for data, labels in test_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        logits = model(data)\n",
    "        \n",
    "        loss_t = loss_function(logits, labels)\n",
    "        acc_t = 100 * torch.sum(torch.argmax(logits,dim = 1) == labels) / len(labels)\n",
    "\n",
    "\n",
    "        loss += loss_t.item()\n",
    "        acc += acc_t\n",
    "        N += data.shape[0]\n",
    "\n",
    "    loss = loss / N\n",
    "    acc = acc / len(test_loader)\n",
    "    \n",
    "    if epoch % output_epochs == 0:\n",
    "        print(f'Epoch: {epoch}, CE = {loss}, ACC = {acc}')\n",
    "  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nm/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "/Users/nm/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, CE = 0.013887106120586395, ACC = 85.83984375\n",
      "Epoch: 1, CE = 0.013362858057022095, ACC = 87.20703125\n",
      "Epoch: 2, CE = 0.015732753075659276, ACC = 85.7421875\n",
      "Epoch: 3, CE = 0.013607118770480157, ACC = 87.20703125\n",
      "Epoch: 4, CE = 0.013322155103087424, ACC = 88.76953125\n"
     ]
    }
   ],
   "source": [
    "dataset = 'mnist'\n",
    "batch_size = 32\n",
    "\n",
    "if dataset == 'digits':\n",
    "    train_loader, test_loader = digits_loader(batch_size = batch_size)\n",
    "    data_width = 8\n",
    "    data_dim = data_width**2\n",
    "    num_classes = 10\n",
    "    output_epochs = 1\n",
    "elif dataset == 'mnist':\n",
    "    train_loader, test_loader = mnist_loader(batch_size = batch_size)\n",
    "    data_width = 28\n",
    "    data_dim = data_width**2\n",
    "    num_classes = 10\n",
    "    output_epochs = 1    \n",
    "\n",
    "    \n",
    "## Define hyperparameters\n",
    "hidden_size = 32 # size of layers in model\n",
    "lr = 0.01 \n",
    "num_epochs = 5\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "### Define model and optimizer\n",
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "model = MLP(input_dim = data_dim, num_classes = num_classes, hidden_dim = hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "best_loss = 1000\n",
    "epochs_since_improvement = 0\n",
    "for epoch in range(num_epochs):    \n",
    "    train_epoch(epoch, train_loader)\n",
    "    test_loss = test_epoch(epoch, test_loader, output_epochs = output_epochs)\n",
    "    if test_loss < best_loss:\n",
    "        best_model = deepcopy(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader\n",
      "tensor([9, 7, 5, 4, 7, 6, 9, 6, 2, 2, 1, 5, 9, 4, 4, 1, 0, 1, 7, 4, 0, 9, 7, 3,\n",
      "        4, 9, 6, 5, 0, 4, 7, 3])\n",
      "tensor([9, 7, 5, 4, 7, 6, 9, 6, 2, 2, 1, 5, 9, 4, 4, 1, 0, 1, 7, 4, 0, 9, 7, 8,\n",
      "        9, 9, 6, 5, 0, 4, 7, 3])\n",
      "tensor(0.1903, grad_fn=<NllLossBackward>)\n",
      "\n",
      "\n",
      "\n",
      "Test Loader\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1, 3, 1])\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1, 3, 1])\n",
      "tensor(0.3076, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('Train Loader')\n",
    "for x,y in train_loader:\n",
    "    output = model(x)\n",
    "    print(torch.argmax(output,dim = 1))\n",
    "    print(y)\n",
    "    print(loss_function(output, y))\n",
    "    break\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('Test Loader')    \n",
    "for x,y in test_loader:\n",
    "    output = model(x)\n",
    "    print(torch.argmax(output,dim = 1))\n",
    "    print(y)\n",
    "    print(loss_function(output, y))\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature kernels\n",
    "\n",
    "# Train set\n",
    "feats_train, labels_train = get_features(model,train_loader)    \n",
    "FK = feats_train @ feats_train.transpose()\n",
    "\n",
    "# Test set\n",
    "feats_test, labels_test = get_features(model,test_loader)    \n",
    "FK_test = feats_test @ feats_test.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 14s, sys: 855 ms, total: 1min 15s\n",
      "Wall time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's decompose the feature kernel. This is kernel PCA on the data with a kernel learned via features\n",
    "U,S,V = np.linalg.svd(FK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 565 ms, sys: 36.4 ms, total: 601 ms\n",
      "Wall time: 324 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Decompose the test kernel just cause we can\n",
    "U_test,S_test,V_test = np.linalg.svd(FK_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.06309123e+06, 4.96130412e+05, 4.57817873e+05, 3.61928555e+05,\n",
       "       3.37971388e+05, 2.19157719e+05, 1.64447879e+05, 9.73574035e+04,\n",
       "       8.12054531e+04, 7.22451590e+04, 4.26400838e+04, 3.75154036e+04,\n",
       "       2.61840257e+04, 2.49301464e+04, 2.16674233e+04, 1.76575909e+04,\n",
       "       1.32244020e+04, 1.10738526e+04, 1.02820863e+04, 9.60930014e+03,\n",
       "       7.49083800e+03, 7.08403254e+03, 6.00716364e+03, 5.19599241e+03,\n",
       "       4.71248047e+03, 4.03342693e+03, 3.54149657e+03, 2.93436721e+03,\n",
       "       2.61389763e+03, 1.66502641e+03, 6.79993712e+02, 2.16706289e+01,\n",
       "       1.11286951e-09])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[0:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.70562037e+05, 9.29771256e+04, 6.32432741e+04, 5.51877225e+04,\n",
       "       5.20200517e+04, 3.47431033e+04, 2.64476912e+04, 1.52996854e+04,\n",
       "       1.16152366e+04, 9.84608900e+03, 6.92901826e+03, 5.44786048e+03,\n",
       "       4.37500968e+03, 3.53152385e+03, 3.31906758e+03, 2.83114999e+03,\n",
       "       2.53105908e+03, 2.06661173e+03, 1.85521915e+03, 1.60679748e+03,\n",
       "       1.48327526e+03, 1.20402944e+03, 9.60334307e+02, 7.50718381e+02,\n",
       "       6.79124394e+02, 5.85725289e+02, 4.62735475e+02, 4.13054296e+02,\n",
       "       3.47637571e+02, 4.62672652e+01, 4.32914034e+01, 1.24818012e-10,\n",
       "       6.42016869e-11])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_test[0:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = best_model['fc3.weight'][:,:].detach().numpy().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we solve F^T F a = w for a\n",
    "# N = # samples = 5000\n",
    "# D = # features = 32\n",
    "# Need feats_train in R^DxN\n",
    "\n",
    "feats_train = feats_train.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving Fa = w, gives a = (F^T F)^-1 F^T w\n",
    "FTF = (feats_train.transpose() @ feats_train) + 0.00001 * np.eye(5000)\n",
    "FTFinv = np.linalg.inv(FTF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTw = (feats_train.transpose() @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = FTFinv @ FTw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.01362504e-02,  4.44235688e-02, -3.37799092e-02, ...,\n",
       "         2.62050346e-03,  3.74421704e-02,  2.03334803e-02],\n",
       "       [ 7.51732377e-04,  2.20430398e-03, -1.89899087e-03, ...,\n",
       "         3.24858899e-03,  2.75285600e-03,  1.30534717e-03],\n",
       "       [-8.19433284e-03,  6.72474991e-04,  4.18309508e-03, ...,\n",
       "         1.07090239e-02,  2.96160493e-03, -1.69968026e-04],\n",
       "       ...,\n",
       "       [-1.54445879e-05, -4.63149045e-05, -7.33952947e-06, ...,\n",
       "        -1.35463866e-04, -1.54428417e-04, -2.07060948e-05],\n",
       "       [ 1.41069293e-04,  2.26218777e-04,  4.33604437e-05, ...,\n",
       "        -7.66542507e-06, -1.43289275e-04, -2.76804494e-05],\n",
       "       [ 1.02874619e-05,  3.96743417e-05,  1.11117879e-05, ...,\n",
       "         2.09433492e-05,  1.14605267e-04, -1.21545745e-05]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.14795407, 1.50111468, 1.04685615, 0.92225615, 1.11397796,\n",
       "       0.99123258, 1.15805069, 1.12715211, 1.08622707, 0.97514829])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.diag(a.transpose() @ FK @ a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
