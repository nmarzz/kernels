{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digits(Dataset):\n",
    "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode='train', transforms=None):\n",
    "        digits = load_digits()\n",
    "        if mode == 'train':\n",
    "            self.data = digits.data[:1000].astype(np.float32)\n",
    "        else:\n",
    "            self.data = digits.data[1000:].astype(np.float32)\n",
    "\n",
    "        self.targets = digits.target\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample, target    \n",
    "    \n",
    "def mnist_loader(batch_size: int) -> None:\n",
    "    train_set = datasets.MNIST('data', train=True,\n",
    "                               transform=transforms.Compose([\n",
    "#                                    transforms.RandomCrop(28, padding=4),\n",
    "#                                    transforms.RandomHorizontalFlip(),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]), download=True)\n",
    "    val_set = datasets.MNIST('data', train=False,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.1307,), (0.3081,))\n",
    "                             ]), download=True)\n",
    "    \n",
    "    train_set = torch.utils.data.Subset(train_set, range(5000))\n",
    "    val_set = torch.utils.data.Subset(val_set, range(1000))\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=batch_size, num_workers=4,\n",
    "        pin_memory=True, shuffle= True)\n",
    "\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory= True)\n",
    "\n",
    "    return train_loader, val_loader    \n",
    "\n",
    "def digits_loader(batch_size:int):\n",
    "    transform = transforms.Lambda(lambda x: 2. * (x / 17.) - 1.)\n",
    "    train_data = Digits(mode='train', transforms=transform)\n",
    "    test_data = Digits(mode='test', transforms=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory= True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader  \n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    ''' A basic 3 layer MLP '''\n",
    "\n",
    "    def __init__(self, input_dim: int, num_classes: int, hidden_dim: int = 32) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes,bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x  \n",
    "    \n",
    "    def features(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(model, loader):\n",
    "    feats = []\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = np.zeros((0, hidden_size))\n",
    "    labels = np.zeros((0))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data = data.to(device)            \n",
    "            embs = model.features(data)\n",
    "            embeddings = np.concatenate((embeddings, embs.cpu().numpy()))\n",
    "            labels = np.concatenate((labels, target))\n",
    "\n",
    "    return embeddings, labels        \n",
    "\n",
    "def train_epoch(epoch, train_loader):    \n",
    "    model.train()      \n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x,y = x.to(device),y.to(device)        \n",
    "        optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = loss_function(output,y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "    \n",
    "        \n",
    "def test_epoch(epoch, test_loader, output_epochs = 10):        \n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    N = 0\n",
    "    for data, labels in test_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        logits = model(data)\n",
    "        \n",
    "        loss_t = loss_function(logits, labels)\n",
    "        acc_t = 100 * torch.sum(torch.argmax(logits,dim = 1) == labels) / len(labels)\n",
    "\n",
    "\n",
    "        loss += loss_t.item()\n",
    "        acc += acc_t\n",
    "        N += data.shape[0]\n",
    "\n",
    "    loss = loss / N\n",
    "    acc = acc / len(test_loader)\n",
    "    \n",
    "    if epoch % output_epochs == 0:\n",
    "        print(f'Epoch: {epoch}, CE = {loss}, ACC = {acc}')\n",
    "  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nm/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "/Users/nm/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, CE = 0.013870636656880379, ACC = 85.83984375\n",
      "Epoch: 1, CE = 0.013568791814148426, ACC = 87.79296875\n",
      "Epoch: 2, CE = 0.014156046748161316, ACC = 87.40234375\n",
      "Epoch: 3, CE = 0.012121602348983287, ACC = 90.625\n",
      "Epoch: 4, CE = 0.013459737211465836, ACC = 87.59765625\n"
     ]
    }
   ],
   "source": [
    "dataset = 'mnist'\n",
    "batch_size = 32\n",
    "\n",
    "if dataset == 'digits':\n",
    "    train_loader, test_loader = digits_loader(batch_size = batch_size)\n",
    "    data_width = 8\n",
    "    data_dim = data_width**2\n",
    "    num_classes = 10\n",
    "    output_epochs = 1\n",
    "elif dataset == 'mnist':\n",
    "    train_loader, test_loader = mnist_loader(batch_size = batch_size)\n",
    "    data_width = 28\n",
    "    data_dim = data_width**2\n",
    "    num_classes = 10\n",
    "    output_epochs = 1    \n",
    "\n",
    "    \n",
    "## Define hyperparameters\n",
    "hidden_size = 32 # size of layers in model\n",
    "lr = 0.01 \n",
    "num_epochs = 5\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "### Define model and optimizer\n",
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "model = MLP(input_dim = data_dim, num_classes = num_classes, hidden_dim = hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "best_loss = 1000\n",
    "epochs_since_improvement = 0\n",
    "for epoch in range(num_epochs):    \n",
    "    train_epoch(epoch, train_loader)\n",
    "    test_loss = test_epoch(epoch, test_loader, output_epochs = output_epochs)\n",
    "    if test_loss < best_loss:\n",
    "        best_model = deepcopy(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader\n",
      "tensor([7, 2, 2, 0, 7, 3, 5, 8, 2, 4, 3, 3, 4, 7, 0, 7, 0, 3, 8, 2, 5, 8, 9, 3,\n",
      "        2, 8, 5, 7, 0, 9, 0, 9])\n",
      "tensor([7, 2, 2, 0, 7, 3, 5, 8, 2, 4, 3, 3, 4, 7, 0, 7, 0, 3, 8, 2, 5, 8, 9, 3,\n",
      "        2, 8, 5, 7, 0, 9, 0, 9])\n",
      "tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "\n",
      "\n",
      "\n",
      "Test Loader\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1, 3, 1])\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1, 3, 1])\n",
      "tensor(0.2855, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('Train Loader')\n",
    "for x,y in train_loader:\n",
    "    output = model(x)\n",
    "    print(torch.argmax(output,dim = 1))\n",
    "    print(y)\n",
    "    print(loss_function(output, y))\n",
    "    break\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('Test Loader')    \n",
    "for x,y in test_loader:\n",
    "    output = model(x)\n",
    "    print(torch.argmax(output,dim = 1))\n",
    "    print(y)\n",
    "    print(loss_function(output, y))\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature kernels\n",
    "\n",
    "# Train set\n",
    "feats_train, labels_train = get_features(model,train_loader)    \n",
    "FK = feats_train @ feats_train.transpose()\n",
    "\n",
    "# Test set\n",
    "feats_test, labels_test = get_features(model,test_loader)    \n",
    "FK_test = feats_test @ feats_test.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at SVD of feature kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Let's decompose the feature kernel. This is kernel PCA on the data with a kernel learned via features\n",
    "# U,S,V = np.linalg.svd(FK)\n",
    "\n",
    "\n",
    "# # Decompose the test kernel just cause we can\n",
    "# U_test,S_test,V_test = np.linalg.svd(FK_test)\n",
    "\n",
    "# print(np.sum(S > 0.01))\n",
    "# print(S[0:33])\n",
    "# print(S_test[0:33])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the norm of projected functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = best_model['fc3.weight'][:,:].detach().numpy().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 5000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we solve F^T F a = w for a\n",
    "# N = # samples = 5000\n",
    "# D = # features = 32\n",
    "# Need feats_train in R^DxN\n",
    "\n",
    "feats_train = feats_train.transpose()\n",
    "feats_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving Fa = w, gives a = (F^T F)^-1 F^T w\n",
    "FTF = (feats_train.transpose() @ feats_train) + 0.00001 * np.eye(5000)\n",
    "FTFinv = np.linalg.inv(FTF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTw = (feats_train.transpose() @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = FTFinv @ FTw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.95756685e-05, -2.45833682e-04, -1.78518228e-04, ...,\n",
       "        -1.73320903e-04,  2.52998187e-04,  1.27209191e-05],\n",
       "       [-3.86567847e-05, -6.13405805e-05, -1.57014127e-04, ...,\n",
       "        -2.00525567e-04,  7.26352636e-05, -6.62693674e-05],\n",
       "       [ 9.79563447e-05, -1.46635444e-04, -4.16743787e-05, ...,\n",
       "        -5.39855282e-05, -2.20317143e-05, -3.14728995e-05],\n",
       "       ...,\n",
       "       [-5.50958066e-05, -1.62754586e-05,  1.41448181e-05, ...,\n",
       "         8.03349030e-06, -9.04596527e-06,  1.74404586e-05],\n",
       "       [-1.50813255e-04,  2.34261155e-04, -7.68830359e-05, ...,\n",
       "         5.61119959e-05, -2.31735816e-04,  2.60820088e-04],\n",
       "       [-8.51559453e-05, -4.71062958e-06,  2.90867756e-05, ...,\n",
       "        -8.92109374e-05,  1.58204930e-05,  7.63103017e-05]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the norms of the projected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.56248519, 2.24821384, 0.81616043, 0.7745437 , 1.27478325,\n",
       "       1.0603234 , 1.55849892, 0.54283823, 0.72376634, 0.65062228])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(a.transpose() @ FK @ a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
